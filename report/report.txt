\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{amsmath}
\usepackage{color}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\graphicspath{{images/}}
\usepackage{subfigure}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} %LaTeX package to import graphics
\graphicspath{{images/}} %configuring the graphicx package
\usepackage[export]{adjustbox}
\usepackage{subfigure}
\usepackage{subcaption}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\DeclareMathOperator{\sign}{sign}
\usepackage[a4paper, total={6in, 8in}]{geometry}

\setcounter{section}{-1}

% Define colors for code highlighting
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Define code listing style
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

% Set code listing style
\lstset{style=mystyle}

\geometry{a4paper, margin=1in}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{rSVD}
\author{Cenzato, Pisante, Procaccio}

\begin{document}
\maketitle

\begin{abstract}
The report should contain a brief description of the numerical methodology and of the code organization. 
The programming architectural choices should be explained as well as the main structure of the code. It should contain the description of some numerical experiment that highlight the effectiveness of the code. It should contain the instructions to run a test case and the expected results. 
\end{abstract}

\section{Introduction}
SVD decomposes $A$ into three matrices, $U$, $\Sigma$, and $V$, ordered by their contributions to $A$. This ordering facilitates efficient approximation by retaining only the most significant components of these matrices.



\newpage

\section{Image Compression}
\subsection{Introduction}
Images can be represented through three color channels: red, green, and blue, each as a $(m \times n)$ matrix with values ranging from 0 to 255. Alternatively, images can be in grayscale, where each pixel is represented by a single intensity value. In the context of compression, we will focus on compressing the matrices representing individual color channels.

To compress a matrix $A$, we seek an approximation requiring significantly less storage space. Randomized Singular Value Decomposition (rSVD) proves advantageous for this purpose. 

For compression, a parameter $k$ is chosen to determine the number of singular values considered for the approximation. A higher $k$ improves approximation quality but also increases the data required for encoding. Additionally, the oversampling factor $p$ can be added to retrieve addiditional information. 

It's worth noting that a similar approach can be employed for grayscale images, where the compression is achieved through the decomposition of a single intensity matrix. This versatility allows for an efficient compression strategy that adapts to both color and grayscale representations.

\subsection{Overview of Image Compression algorithm}
The C++ code utilizes Singular Value Decomposition (SVD) for image compression, setting parameters for rank ($r$) and oversampling ($p$). It loads a PNG image using the STB library, instantiates the `APPLICATIONS` class, and measures compression time. The code adapts to RGB or grayscale, applying rSVD-based compression accordingly. After compression, a backward conversion restores the image. Key metrics like uncompressed size, compressed size, ratio, and duration are computed. The decompressed image is saved, and memory is freed. The pseudocode succinctly outlines these steps in the \texttt{main} function.
\begin{algorithm}
\caption{Main Function for Image Compression}
\begin{algorithmic}[1]
\Procedure{main}{}
    \State Set compression parameters $r$ and $p$
    \State Set input and output file paths
    \State Load input image from file
    \If{RGB image}
        \For{each color channel}
            \State Perform image compression using rSVD
            \State Perform backward conversion to obtain decompressed image
        \EndFor
    \Else
        \State Perform image compression using rSVD (grayscale)
        \For{each color channel}
            \State Perform backward conversion to obtain decompressed image
        \EndFor
    \EndIf
    \State Save decompressed image to file
    \State Free allocated memory
    \State \textbf{return} 0
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Testing and results}
Testing was done for the nicest and most wellness picture in the entire world.


\begin{figure}[h]
  \centering

  \begin{subfigure}{0.2\textwidth}
    \includegraphics[width=\textwidth]{luna_rossa.png}
    \caption{Immagine input}
    \label{fig:sub1}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{luna_rossa_out10010.png}
    \caption{r = 100, p = 10}
    \label{fig:sub2}
  \end{subfigure}

  \vspace{\baselineskip} % Aggiunge uno spazio verticale tra le due coppie di immagini

  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{luna_rossa_out5010.png}
    \caption{r = 50, p = 10}
    \label{fig:sub3}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{luna_rossa_out1210.png}
    \caption{r = 12, p = 10}
    \label{fig:sub4}
  \end{subfigure}

  \caption{Quattro immagini a due a due}
  \label{fig:all}
\end{figure}




\section{PCA}

\subsection{Introduction}
Principal Component Analysis (PCA) is a widely used technique in the field of multivariate analysis and statistics. It provides a powerful tool for extracting essential information from high-dimensional datasets by identifying and capturing the most significant patterns within the data.

PCA aims to transform the original variables of a dataset into a new set of uncorrelated variables called principal components. These components are ordered by their significance, with the first component explaining the most variance in the data. By selecting a subset of the top principal components, one can achieve dimensionality reduction while retaining the essential features of the dataset. In this test, we explore the application of PCA as a dimensionality reduction method and its role in uncovering the latent structures and relationships present in a given dataset.

\subsection{Overview of the PCA algorithm}
For the implementation of PCA we use a randomized Singular Value Decomposition (rSVD) approach. The rSVD algorithm leverages randomization techniques to efficiently approximate the principal components of a matrix, making it suitable for large-scale datasets. Additionally, we preprocess the data by centering it before applying the rSVD. 

\begin{algorithm}
\caption{Principal Component Analysis (PCA)}
\begin{algorithmic}[1]
\Function{pca}{$A, r$}
    
    \If{$m > n$}
        \State $X \gets A$
    \Else
        \State $X \gets A.\text{transpose()}$
    \EndIf

    \For{$i \gets 0$ \textbf{to} $m-1$}
        \State Compute mean for each row
        \State Subtract the mean over row to every element of the row
    \EndFor

    \State $(U, s, V) \gets \text{obj.rsvd}(X, r)$

    \If{$using A$}
            \State \textbf{return} $U \cdot s.\text{asDiagonal}()$
    \Else \Comment{If using $A^\top$}
        \State \textbf{return} $U.\text{transpose()} \cdot X$
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}


\subsection{Test results}
In particular, we performed the principal component analysis (PCA) on a dataset from the FDA-NCI Clinical Proteomics Program Databank.

Each column of the dataset represents measurements taken from a patient. There are 216 columns representing 216 patients, out of which 121 are ovarian cancer patients and 95 are normal patients.

Each row represents the ion intensity level at a specific mass-charge value indicated in MZ. There are 2000 mass-charge values, and each row represents the ion-intensity levels of the patients at that particular mass-charge value.

\vspace{\baselineskip}
This dataset has an extensive array of features, making a comprehensive analysis challenging. To address this complexity, we applied Principal Component Analysis (PCA).

As illustrated in {figure} [h], the singular values exhibit a rapid decline. Moreover, we observe that the first three singular values explain nearly 90\% of the dataset's variance.

\begin{figure}[h]
\centering
\includegraphics[width=0.55\linewidth]{pca_explained_variance200.png}
\caption{\label{pca1}plot of the explained variance of the dataset.}
\end{figure}

Observing the scatterplots presented in Figures 2 and 3, it becomes evident that a separation between patients with and without cancer is achieved by considering only the first two or three principal components. This highlights the effectiveness of dimensionality reduction through Principal Component Analysis (PCA) in elucidating patterns indicative of cancer presence in the dataset.

\begin{figure}
\centering
\includegraphics[width=0.35\linewidth]{pca_2scatterplot.png}
\caption{\label{pca2}scatterplot of the first 2 principal components.}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.35\linewidth]{pca_3scatterplot.png}
\caption{\label{pca3}scatterplot of the first 3 principal components.}
\end{figure}




\section{Singular Value Decomposition}
Having explored the principles and applications of Principal Component Analysis (PCA) and Image Compression, we now turn our attention to the underlying mathematical technique : Singular Value Decomposition (SVD). It is a matrix factorization technique that decomposes a given matrix into three separate matrices, revealing essential information about the structure and properties of the original matrix.

For a given matrix $A \in \mathbb{R}^{m \times n}$, the SVD factorization is expressed as:
\begin{align*}
    A & = U \Sigma V^T \\
\end{align*}
where:
\begin{align*}
    U & : \text{Left singular vectors matrix} \in \mathbb{R}^{m \times m} \\
    \Sigma & : \text{semi-Diagonal matrix of singular values} \in \mathbb{R}^{m \times n} \\
    V^T & : \text{Transpose of right singular vectors matrix} \in \mathbb{R}^{n \times n}
\end{align*}

The diagonal elements of $\Sigma$ represent the singular values, and the columns of $U$ and $V$ are the left and right singular vectors, respectively. Notably, SVD is applicable to any matrix, regardless of its size or properties.


\subsection{SVD with Power Method}
The SVD can be computed using the Power Method. The latter is an algorithm which converges to the eigenvector that corresponds to the maximal eigenvalue of the matrix taken into consideration.

\vspace{\baselineskip}
    The pseudo-algorithm used to compute the SVD is the following:

\begin{algorithm}
\caption{SVD with Power Method}
\begin{algorithmic}[3]
\Function{SVDwithPM}{$A$}

    \While{$i < k$}
        \State $B \gets A^\top \cdot A$
        \State $v \gets \text{PowerMethod}(B)$
        \State $\sigma \gets \lVert A \cdot v \rVert$

        \If{$\sigma > \epsilon$}
            \State $u \gets A \cdot v \cdot (1 / \sigma)$
            \State $A \gets A - (\sigma \cdot u \cdot v^\top)$
            \State $i \gets i + 1$
        \EndIf
    \EndWhile

    \State \textbf{return} $(U, s, V)$
\EndFunction
\end{algorithmic}
\end{algorithm}

Being \( k = \min(m, n) \), where \( m \) and \( n \) are the dimensions of the matrix.

The iterations continue until one of the following conditions is met:

1. If the matrix is of full rank, iterate until reaching the minimum dimension of the matrix (\( k \)).

2. If the computed eigenvalue is almost equal to zero (up to a given tolerance), stop the iteration.

\vspace{\baselineskip}
As for the power method I first considered the following algorithm:
\begin{algorithm}
\caption{Power Method for Eigenvalue Estimation}
\begin{algorithmic}[1]
\Function{PowerMethod}{$A$}
    \State Initialize $v$ as a Gaussian vector with $\|v\| = 1$
    \State Compute $B \gets A^\top \cdot A$
    \State $err \gets \infty$
    
    \While{$err > \epsilon$}
        \State $v \gets B \cdot v$
        \State $v \gets \frac{v}{\|v\|}$
        \State $err \gets \|v_{\text{old}} - v\|$
    \EndWhile

    \State \textbf{return} $v$
\EndFunction
\end{algorithmic}
\end{algorithm}

Let \(v_i\) be the eigenvector computed at iteration \(i\) and \(v_{i-1}\) be the eigenvector computed at the previous iteration. The stopping criteria for the algorithm is defined as follows:

\[ \lVert v_i - v_{i-1} \rVert < \epsilon \]

where \(\lVert \cdot \rVert\) represents the norm of a vector, and \(\epsilon\) is a given tolerance value.

\vspace{\baselineskip}
However, this algorithm requires the computation of the matrix \(B = A^T \cdot A\), which can pose challenges since the computation of the multiplication is very expensive. To address these issue, an alternative algorithm is considered, focusing only on the matrices \(A\) and \(A^T\).

\begin{algorithm}
\caption{Power Method for Singular Value Decomposition}
\begin{algorithmic}[1]
\Function{PowerMethod}{$A$}
    \State Initialize $v$ as a Gaussian vector of size $A.\text{rows()}$ with $\|v\| = 1$

    \While{$err > \epsilon$}
        \State $v_{\text{old}} \gets v$
        \State $u \gets A \cdot v$
        \State $u \gets \frac{u}{\|u\|}$
        \State $v \gets A^\top \cdot u$
        \State $v \gets \frac{v}{\|v\|}$
        \State $err \gets \|v_{\text{old}} - v\|$
    \EndWhile

    \State \textbf{return} $(u, v)$
\EndFunction
\end{algorithmic}
\end{algorithm}



\subsection{rSVD}
   Another algorithm implemented in this class is the randomized SVD. This algorithm is particularly advantageous for analyzing large matrices as it reduces dimensionality. Given a matrix \(A\) of size \(m \times n\) and a target rank \(r\), the algorithm proceeds as follows:

\begin{algorithm}
\caption{Randomized SVD (rSVD) Algorithm}
\begin{algorithmic}[1]
\Function{rsvd}{$A, r, p, q$}
    \State $P \gets \text{generate a gaussian matrix of dimensions n x k}$ 
    
    \State $U \gets A \cdot P$ \Comment{$U$ is of size $m \times k$}

    \For{$i \gets 0$ \textbf{to} $q-1$}
        \State $U \gets A \cdot (A^\top \cdot U)$
    \EndFor

    \State $(Q, U) \gets \text{QR.Givens\_solve\_parallel}(U)$
    \State $\text{QR.setQR\_for\_svd\_parallel}(Q, U)$

    \State $Q \gets Q.\text{topLeftCorner}(m, k)$

    \State $Y \gets Q^\top \cdot A$ \Comment{$Y$ is of size $k \times n$}

    \State $(U_y, s, V) \gets \text{svd\_with\_PM}(Y)$
    \State $U \gets Q \cdot U_y$

    \State \textbf{return} $(U, s, V)$
\EndFunction
\end{algorithmic}
\end{algorithm}

   The resulting SVD is an approximation of the original matrix $A$, with only the first $r$ singular values and corresponding singular vectors computed.

\subsection{Gaussian matrix}
The \texttt{SVD::genmat} function generates a random matrix of dimensions $m \times n$ filled with values sampled from a standard normal distribution (with mean $0$ and standard deviation $1$). The function utilizes OpenMP for parallelization to enhance computational efficiency.

\begin{itemize}
    \item \textbf{Matrix Initialization:} The function initializes a matrix $M$ of size $m \times n$ to store the generated random values.
    
    \item \textbf{Parallelization with OpenMP:} The function utilizes OpenMP directives to parallelize the loop responsible for generating random values.
    
    \item \textbf{Thread-Specific Random Seeds:} Within the parallel section, each thread obtains a unique random seed by adding its thread rank to the value obtained from \texttt{std::random\_device}. This ensures that each thread generates distinct random values.
\end{itemize}

\subsubsection*{MPI Parallelized Matrix Generation}

The \texttt{genmat} function employs MPI parallelization to generate a random matrix distributed among different processes. It calculates the distribution of rows among processes, and generates local matrices on each process. Random values are sampled from a standard normal distribution, and the local matrices are gathered to construct the global matrix.


\subsubsection*{Comments}

An observation from our experiments reveals that the MPI version outperformed the OpenMP implementation when dealing with large matrix dimensions, such as $5000 \times 5000$. However, it is noteworthy that our typical application scenarios involve smaller matrices, typically on the order of $1 \times 10^{-3}$.

The parallelization choices were made to optimize the performance of the function in diverse computing environments. While the MPI version demonstrated superior efficiency with larger matrices, the OpenMP approach remains relevant for smaller matrices, balancing the trade-off between computational resources and scalability.


\subsection{Results}
Several tests were conducted to validate the efficiency and correctness of the proposed methods. The tolerance \(\epsilon\) was set to \(10^{-8}\), and the tests were performed on matrices of increasing size generated randomly within the range of -10 to 10.

\begin{enumerate}
\item $10 \times 10$ matrix

SVD with Power Method:

Time of execution power method 1-st algorithm: 0.00329849 s

\( \text{PM1} : \| A - U \Sigma V^T \| = 1.05767 \times 10^{-14} \)

Time of execution power method 2-nd algorithm: 0.004897 s

\( \text{PM2} : \| A - U \Sigma V^T \| = 1.11608 \times 10^{-14} \)

Difference eigenvalues = 8.98115e-15

\vspace{\baselineskip}
rSVD ( r = 5 ):

Time of execution rSVD algorithm: 0.00694928 s

\(\| A - U \Sigma V^T \| = 19.3972 \)

Speed Up randomized = 0.474652

Norm of difference first r eigenvalues = 0.235447

\vspace{\baselineskip}
\item $100 \times 100$ matrix

SVD with Power Method:

Time of execution power method 1-st algorithm: 3.36989 s

\( \text{PM1} : \| A - U \Sigma V^T \| = 3.00817 \times 10^{-13} \)

Time of execution power method 2-nd algorithm: 2.95494 s

\( \text{PM2} : \| A - U \Sigma V^T \| = 3.15329 \times 10^{-13} \)

Difference eigenvalues = 1.62896e-13

\vspace{\baselineskip}
rSVD ( r = 60 ):

Time of execution rSVD algorithm: 1.27542 s

\(\| A - U \Sigma V^T \| = 148.793 \)

Speed Up randomized: 2.64219

Norm of difference first r eigenvalues = 0.179286

\vspace{\baselineskip}
\item $1000 \times 200$ matrix

SVD with Power Method:

Time of execution power method 1-st algorithm: 65.0099 s

\( \text{PM1} : \| A - U \Sigma V^T \| = 1.86931 \times 10^{-12} \)

Time of execution power method 2-nd algorithm: 532.937 s

\( \text{PM2} : \| A - U \Sigma V^T \| = 2.00974 \times 10^{-12} \)

Difference eigenvalues = 1.53411e-12

\vspace{\baselineskip}
rSVD ( r = 160 ):

Time of execution rSVD algorithm: 43.7981 s

\(\| A - U \Sigma V^T \| = 908.887 \)

Speed Up randomized: 1.48431

Norm of difference first r eigenvalues = 0.436731
\end{enumerate}
\vspace{\baselineskip}

From these tests it can be seen that:
\begin{itemize}
\item In the algorithm using the Power Method, the error in the reconstruction of the matrix from the Singular Value Decomposition (SVD) is consistently low, approaching the machine epsilon, across all conducted tests. This suggests a highly accurate factorization. Additionally, the two distinct Power Methods produce nearly identical eigenvalues.

\item In the randomized SVD (rSVD) method, the error in the factorization is comparatively higher. However, this discrepancy is deemed acceptable, given the nature of rSVD as an approximation method. The primary objective of rSVD is to expedite computation, and the introduced error is considered reasonable in light of this objective. Indeed, a speedup is observed in comparison to the first algorithm utilizing the power method (the speedup is clearly dependent on the target rank used in the rSVD). Furthermore, the computed eigenvalues in the randomized approach demonstrate a high degree of similarity to those obtained through a non-randomized methodology.

\item In the third test, it becomes evident that the initial algorithm employed for the power method performs better when applied to non-square matrices. This is attributed to its capability to effectively reduce dimensionality in such scenarios.

\item Finally, the correctness of the algorithms has been tested by comparing the results obtained from the SVD with the ones obtained using python.

\end{itemize}



\section{QR Decomposition}

QR factorization is a fundamental numerical technique used in linear algebra to decompose a matrix into the product of an orthogonal matrix and an upper triangular matrix. This factorization plays a crucial role in various applications, including solving systems of linear equations, least squares problems, and eigenvalue computations.

The decomposition of a matrix $A$ into the product $A = QR$ involves obtaining an orthogonal matrix $Q$ and an upper triangular matrix $R$. The orthogonal matrix $Q$ captures the rotation and reflection components, while the upper triangular matrix $R$ contains the essential information about the original matrix.

In this report, we delve into the QR factorization process, specifically focusing on two distinct algorithms: the Givens rotation and the Householder reflection. These algorithms provide numerical methods to transform a given matrix into its QR factorization, each with its unique approach to introducing zeros below the main diagonal.

The Givens algorithm employs sequential rotations to eliminate elements below the diagonal, while the Householder algorithm uses successive reflections to achieve a similar result.

In the subsequent sections, we provide an in-depth overview of our version of the Givens and Householder algorithms, discussing their implementation details, computational considerations, and performance characteristics.


\subsection{Overview of Givens Algorithm}

The Givens algorithm is a numerical technique used for QR factorization. The Givens rotation is employed to introduce zero elements below the diagonal of the matrix, gradually transforming it into its triangular form.
\begin{algorithm}
\caption{Givens QR Factorization}
\begin{algorithmic}[1]
\Procedure{GivensQR}{$A$}
    \State $m, n \gets \text{dimensions of } A$
    \State Initialize $Q$ as identity matrix
    \State $R \gets A$
    
    \For{$j \gets 1$ \textbf{to} $n$}
        \For{$i \gets m$ \textbf{downto} $j+1$}
            \State Compute Givens rotation parameters $c, s$ for $R(i-1, j), R(i, j)$
            \State Apply Givens rotation to update $R$ and $Q$
        \EndFor
    \EndFor
    
    \State \textbf{return} $Q, R$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{Givens Algorithm with OpenMP}

The OpenMP implementation of the Givens algorithm effectively parallelizes computations across multiple threads. This approach involves parallelizing the outer loop over matrix columns (\(j\)), enabling concurrent computation on independent columns. Thread-specific variables, including \(c\), \(s\), \(a\), \(b\), and \(tmp\), are marked as private to ensure thread safety and prevent potential data races. The use of \texttt{\#pragma omp single} guarantees that critical initial calculations are executed by a single thread, avoiding race conditions. Atomic addition (\(tmp = 0.0;\)) is employed to prevent data races when updating shared variables within the parallel region. Computation on matrices \(Q\) and \(R\) is efficiently integrated within the parallel region, eliminating the need for additional synchronization mechanisms.

\subsubsection{Givens Algorithm with MPI}

The MPI implementation of the Givens algorithm adopts a distributed computing approach, distributing matrix columns across multiple processes. This is achieved by leveraging the message passing property of MPI. Each process iterates over different columns and starts executing only after the \((j-1)\)-th column has computed the \((m-1)\)-th and \((m-2)\)-th rows. This is shown in {figure}[h] .



Synchronization of the computation involves exchanging flags (\texttt{syncflag}) to coordinate the execution among processes. The use of \texttt{MPI\_Scatterv} efficiently distributes portions of the original matrix (\(R\)) to each process. This operation is particularly effective in minimizing communication overhead and ensuring efficient data movement. Similarly, \texttt{MPI\_Gatherv} is employed to collect the results from each process and reconstruct the final matrix, completing the distributed computation.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Applying-Givens-rotations-in-parallel.png}
    \caption{Visualization of the MPI Givens algorithm.}
    \label{fig:mpi_givens}
\end{figure}



\subsubsection{General Observations and Recommendations}

Both implementations exhibit a noticeable speedup starting from an average matrix size of 100 when executed on a standard PC with the specifications attached at the end of the report.

The MPI approach theoretically aims for a speedup of \( \frac{n}{2} \), although the actual achieved speedup is likely lower due to the substantial overhead introduced by message passing. Despite this, the MPI implementation demonstrates superior scalability compared to the OpenMP approach.

Below are the presented results:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{grafico.pdf}
    \caption{Speedup comparison between MPI and OpenMP implementations.}
    \label{fig:speedup_comparison}
\end{figure}

These results highlight a significant improvement in performance, particularly for larger matrix sizes. The MPI approach showcases a more scalable behavior, confirming its suitability for parallelizing the Givens algorithm.



\subsubsection{Results and Performance}
The implementation avoids explicitly constructing the Givens rotation matrix and directly computes the changes to matrices $Q$ and $R$ during each iteration. This approach can be computationally efficient, especially for large matrices.



\subsection{Overview of Householder Algorithm}

The Householder algorithm is another numerical technique used for QR factorization, a process that decomposes a matrix into the product of an orthogonal matrix (Q) and an upper triangular matrix (R). Similar to the Givens algorithm, the Householder transformation is employed to introduce zero elements below the diagonal of the matrix.

In the implementation provided, the Householder algorithm is applied to a given matrix $A$ to obtain its QR factorization, where $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix.

\begin{algorithm}
\caption{Householder QR Factorization}
\begin{algorithmic}[1]
\Procedure{HouseholderQR}{$A$}
    \State $m, n \gets \text{dimensions of } A$
    \State Initialize matrices $Q$ and $R$
    \State $Q \gets I_m$ \tcp{Initialize $Q$ as identity matrix}
    \State $R \gets A$
    
    \For{$j \gets 1$ \textbf{to} $\min(m, n)$}
        \State Compute Householder vector $v$ from $R(j:m, j)$
        \State Compute Householder transformation matrix $P$
        \State Update $R$ and $Q$ using $P$
    \EndFor
    
    \State \textbf{return} $Q, R$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsubsection{Parallelization}
The parallelized Householder QR factorization algorithm begins by initializing matrices for orthogonal transformations \([Q]\), rotation matrices \([P]\), and the input matrix \([R]\). It employs OpenMP parallelization to enhance efficiency, distributing the computation of vectors and matrices across multiple threads. The algorithm iteratively computes Householder vectors, transforming the input matrix into upper triangular form. The computed orthogonal transformations \([Q]\) and upper triangular matrix \([R]\) are updated concurrently within parallel sections, exploiting parallelism for accelerated computation. To improve numerical stability, the algorithm normalizes the Householder vectors and applies the transformations to the input matrix. Additionally, a parallel loop efficiently zeroes out the lower triangular portion of the resulting upper triangular matrix. The algorithm utilizes critical sections to ensure correct matrix updates within parallel regions.

Such implementation exhibits a good speedup, as shown in the figure below.


\subsection{Computational Considerations}
One can assert that, in general, for dense matrices, it is advantageous to use the Householder algorithm for QR factorization. On the other hand, if the matrix to be factorized already has many zero diagonals below the main diagonal, then the Givens algorithm may be more convenient from a computational perspective. This is based on both the reduced computation time and the fewer number of operations performed. The choice between Householder and Givens algorithms is influenced by the matrix structure and the specific requirements of the computation. Householder transformations are preferred for their numerical stability and efficiency in handling dense matrices, while Givens rotations are advantageous when dealing with matrices exhibiting sparsity, particularly with many zero diagonals below the main diagonal. Choosing the Givens algorithm is advantageous for scenarios that demand sparsity preservation, numerical stability, and incremental updates, but careful consideration is necessary for optimal performance in specific problem contexts.


\section{MATRIX IMPLEMENTATION}

In order to perform the operations described above, a custom \texttt{Matrix} class has been developed.

The class is generic with respect to the type saved in the matrix (preferably a floating point), and it has another template parameter which is the ordering, either row-major or col-major.

The decision of the ordering is done at compile time, so the implementation has the choice to select the best implementation with no overhead at runtime.

The \texttt{FullMatrix} class has been developed in parallel with all the algorithms, so the first implementation contained the matrix classes provided by the Eigen library.

In order to avoid the rewriting of the code, all the methods provided by the \texttt{FullMatrix} class have been created to emulate most of the Eigen functionalities. Indeed, if a particular method was utilized from the \texttt{Eigen::MatrixXd} class, it would appear a few days later also in the \texttt{FullMatrix} class.

\textbf{Note:} most of the functionalities are mirrored, but some could not be exactly replicated due to the inherently different structure (and surely, more complex) of the Eigen library. So in some parts of the code, there is a preprocessor directive:

\begin{verbatim}
#ifdef EIGEN

// Eigen::MatrixXd

#else 

// FullMatrix

#endif
\end{verbatim}

\textbf{Note:} Eigen uses another class for the definition of a vector, indeed \texttt{Eigen::VectorXd}. But the \texttt{FullMatrix} class can be also used as a vector; indeed, in the file \texttt{utils.hpp} there is:

\begin{verbatim}
using Vector = FullMatrix<Real, ORDERING::ROWMAJOR> // Real is a double
\end{verbatim}

\textbf{Remember:} there is an overload of the constructor of \texttt{FullMatrix} defined as

\begin{verbatim}
FullMatrix(const size_t n): FullMatrix(n,1) {}
\end{verbatim}

which creates a column vector.

So the accessing can be performed both in a matrix way, using the \texttt{operator()}, and in a vector way, using the \texttt{operator[]}.


\subsection{TRANSPOSITION}

The first implementation, when calling the method \texttt{transpose()}, would return a copy of the matrix but with the entries rearranged in order to satisfy the transposition.

This, of course, is not a good implementation since we are creating another matrix with all the information already present in the original one. With just one change of perspective, we can achieve much better performances.

Indeed, a class \texttt{FullMatrixAdjoint} has been developed that contains a reference to a \texttt{FullMatrix} object. When calling the access operators on a \texttt{FullMatrixAdjoint} object, it returns the elements in the matrix but from a transposed point of view (everything is mirrored with respect to the original object).

Note that this class is not a child of a \texttt{FullMatrix}, but a wrapper. This decision was made because conceptually a parent should not call methods that return its child, which happens in the \texttt{transpose()} method. But, of course, this decision has its disadvantages, since each method has to be re-declared from scratch (here in the class only the necessary methods have been developed).

The more correct solution would be similar to the Eigen implementation, each matrix class should derive from a superclass (called \texttt{MatrixBase} in Eigen) where most of the methods are defined.

Small note: there is some code repetition in the matrix class, since the matrix multiplication needs to be specified for matrix-transposed and the reverse, and for transposed-expression and reverse.

A better solution is to perform a partial specialization of the method with a concept (defined for matrix and its transposed), but there was a problem since I could not create a concept for the \texttt{FullMatrixTransposed} class. The type should be something along the lines of \texttt{FullMatrix<Real, ORDERING>::FullMatrixAdjoint}, but no matter the template I created, the compiler continued to give an error for the type declaration of the class.

\subsection{MULTIPLICATION}

The matrix can perform most of the classical component-wise operations (see section on lazy evaluation) and row-column operations.

Then the most expensive operation with matrices is the matrix-matrix multiplication, and here we briefly discuss some results.

The operation has been constructed following the article: \url{https://siboehm.com/articles/22/Fast-MMM-on-CPU}. Almost everything described has been implemented, except for the tiling of the operation.

The implementation is different for row-major and col-major matrices, and for both cases, it tries to exploit completely cache coherency and SIMD operations.

The results below show the performance of the matrix-matrix multiplication for increasing sizes of the matrices, both without compiler optimization and with the flags \texttt{"-O3 -march=native"} activated.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{matmult_noopt.png}
    \caption{Photo of matmult\_noopt.png}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{matmult_opt.png}
    \caption{Photo of matmult\_opt.png}
\end{figure}

Here we can notice two main properties:

\begin{itemize}
    \item the optimizer does a lot of work (for \(N=1000\) we get approximately 70x speedup).
    \item up until approximately \(N=1600\) the OpenMP implementation keeps the pace with Eigen (which has also OpenMP beneath), but after that, there is a huge loss in performance, since the implementation does not perform tiling and the matrices are too big to fit in a single page of the cache.
\end{itemize}

\subsection{LAZY EVALUATION}

The matrix can also perform component-wise operations in a lazy way, by setting the compilation flag \texttt{"-DLAZY"}.

The pattern used is the CRTP (curiously recursive template pattern) of the expression templates, described extensively [here](https://link.springer.com/article/10.1007/s00791-009-0128-2).

This pattern helps construct an expression class at compile time that contains the expression tree evaluated by the compiler. Indeed, each class derives from a superclass \texttt{Expr<T>} recursively in the following way:

\begin{verbatim}
public class C : Expr<C> {…}
\end{verbatim}

There was also the possibility to construct lazily the row-column product, but this implementation was held back mainly by two major problems:

\begin{itemize}
    \item the optimization for cache-friendly accesses would require the multiplication operator (defined as a functor) to know about the ordering of the matrix at compile time and to reorder the operations accordingly.
    \item there is a major problem when performing an operation such as:

    \begin{verbatim}
    FullMatrix a;

    a = a * a; // Assuming that a is squared
    \end{verbatim}

    If the operation is performed in a lazy way, the result will, of course, not be correct starting from the second term computed in the matrix.

    An alternative solution then would require understanding at compile time if a matrix is present both in the lhs and the rhs of the operation, but the solution is not trivial and requires traversing the expression tree recursively at compile time.

    A simpler solution could also be to create a temporary each time the multiplication operator between matrices (or expressions of matrices) is called, but this is exactly what is done without expression templates. This would also not impact the performance much since creating a matrix is less costly than performing a multiplication.

    So in the end this operation was kept without lazy evaluation.
\end{itemize}

\textbf{Note:} the same problem emerges when doing an operation such that

\begin{verbatim}
a = a + (or -) a.transpose();
\end{verbatim}

Which returns the symmetric (or skew-symmetric) part of a matrix, but since it is not required from the application it was not addressed.

The tests in order to understand the improvements were performed on 5 different operations, both in a lazy and non-lazy way, all with the optimization flags enabled:

\begin{itemize}
    \item \texttt{c = a + b; // SUM\_SHORT}
    \item \texttt{c = a + b + a + b + a + b + a + b + a + b; // SUM\_LONG}
    \item \texttt{c = a * 2.; // PROD\_SHORT}
    \item \texttt{c = a * 2. * 2. * 2. * 2. * 2. * 2. * 2. * 2. * 2.; // PROD\_LONG}
    \item \texttt{c = a * (a + b * 2.) * a.transpose(); // MIXED}
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{sumshort_lazy.png}
    \caption{Photo of sumshort\_lazy.png}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{sumshort_nolazy.png}
    \caption{Photo of sumshort\_nolazy.png}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{sumlong_lazy.png}
    \caption{Photo of sumlong\_lazy.png}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{sumlong_nolazy.png}
    \caption{Photo of sumlong\_nolazy.png}
\end{figure}

Here we can see that using lazy evaluation on the sum of matrices has a huge impact since for the long operation the speedup reaches 25x.

Now since this operation is not very realistic, we can see that only with a small operation we get an approximate 2.5x speedup, and since is just 1 against the 9 sums of before, the results are expected (we create 9 temporaries and do a last assignment -> 10 operations in total).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{prodshort_lazy.png}
    \caption{Photo of prodshort\_lazy.png}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{prodshort_nolazy.png}
    \caption{Photo of prodshort\_nolazy.png}
\end{figure}

Also for the product of multiple scalars, we get a similar speedup, this time is actually higher since we have to do fewer accesses to the memory because we work only with one matrix.

Interestingly, we can see that for the lazy product, an OpenMP implementation gives some performance increase, not much but it is still approximately a 1.1x speedup.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{mixed_nolazy.png}
    \caption{Photo of mixed\_nolazy.png}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{mixed_lazy.png}
    \caption{Photo of mixed\_lazy.png}
\end{figure}

For a mixed operation, we can see that the graph very much resembles the matrix-matrix multiplication, just with a lot more time since now we perform 3 multiplications.

So if we would like an improvement from the timing perspective, we would need to increase much more the size of the lazy expression, but it is not realistic since it is not in any of the algorithms implemented. But at least there is an improvement in memory usage, since fewer temporaries are created.

\subsection{BENCHMARKS}

The specifics of my computer are the following:

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{my_specs.png}
    \caption{Photo of my\_specs.png}
\end{figure}

All the tests are performed using a benchmark that executes the operations a bunch of times for different sizes of the matrices.

Then the best-performing time (the fastest) is kept after all the tests have been done and stored in a CSV file.

The formula for how many iterations have to be performed is the following exponential:

\[\text{times} = \text{floor}(2. + (\text{end} – 2.) \times \exp\left(\]

where \(i\) is the current dimension of the matrix, \(\text{{start}}\) is the initial dimension, \(\text{{end}}\) is the final dimension, and \(\tau\) is a constant decided differently for each operation.

\textbf{Small note:} In the code, there is also a linear interpolator and a cubic interpolator, but the cubic does not work well, and the linear does not reflect well the difference in time/size with the matrix multiplication (since it grows as a cubic function).

\subsection{For the Future}

There are other possibilities for growing the project even more. Here are some ideas:

\begin{itemize}
    \item Doing tiling in matrix-matrix multiplications (or even using GPUs, with CUDA).
    \item Completing the section on lazy evaluations by adding the lazy row-column product where the decision of storing it in a temporary or not is performed at compile time.
    \item Introducing lazy evaluation with ranges: in this [article](https://gieseanw.wordpress.com/2019/10/20/we-dont-need-no-stinking-expression-templates/), there is an informal description of how we can perform lazy evaluation with one of the newest introductions in C++, the ranges. It shows also how to perform matrix-vector multiplication, but the matrix was stored in a 2D array. An interesting starting point would be the function \texttt{std::ranges::view::chunk} and \texttt{chunk\_view} (described [here](https://en.cppreference.com/w/cpp/ranges/chunk_view)) to split the 1D vector into chunks of equal size. However, this implementation requires C++ 23, which is only supported by the newest compiler versions, and we would need to change all the versions of our current compiler. I personally do not think that we can get any substantial improvement with respect to the expression templates implementation, but it can be very interesting to study a more "functional" and modern approach to the problem, since there is not a lot of literature on this topic yet.
\end{itemize}


\end{document}